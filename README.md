
Markdown

# Comment Intelligence (TikTok)

A small full-stack project that scrapes TikTok comments, cleans them up, runs sentiment + topic modeling, and lets you query them with hybrid RAG (BM25 + pgvector) â€” all with a FastAPI backend and a Streamlit UI.

---

### TL;DR

-   **Stack**: FastAPI Â· Streamlit Â· PostgreSQL + pgvector Â· LangChain Â· BERTopic Â· SentenceTransformers
-   **What it does**: Scrape TikTok comments â†’ Preprocess (Indonesian) â†’ Sentiment Analysis â†’ Topic Modeling â†’ Hybrid RAG Search â†’ Persist to DB & Files.

## âœ¨ Screenshots

*(Replace these placeholders with your actual screenshots)*

| Pipeline Analysis Page                                     | RAG Query Page                                         |
| ------------------------------------------------------------- | --------------------------------------------------------- |
| ![Pipeline Page](https://via.placeholder.com/400x300.png?text=Pipeline+Page) | ![RAG Page](https://via.placeholder.com/400x300.png?text=RAG+Query+Page) |

## ğŸš€ Key Features

-   **Scrape TikTok Comments**: Uses Apify to fetch comment data from TikTok videos.
-   **Text Preprocessing**: Tailored for Indonesian text (cleans URLs, mentions, slang, etc.).
-   **Sentiment Analysis**: Utilizes an `IndoBERT` model for sentiment classification (positive, negative, neutral).
-   **Topic Modeling**: Employs `BERTopic` to identify and label the main topics within the comments.
-   **Hybrid RAG Search**: Smart search combining BM25 (lexical search) and `pgvector` (semantic search), with answers generated by Qwen (optional).
-   **Data Persistence**: Saves analysis results as files (CSV, JSON, TXT) and persists them in a PostgreSQL database.

## ğŸ› ï¸ Tech Stack

-   **Backend**: FastAPI
-   **Frontend**: Streamlit
-   **Database**: PostgreSQL + pgvector
-   **Orchestration & LLM**: LangChain
-   **AI Models**: BERTopic, SentenceTransformers, IndoBERT
-   **Deployment**: Docker & Docker Compose

## ğŸ“‹ Prerequisites

Before you begin, ensure you have the following:

1.  **Docker & Docker Compose**: Highly recommended for easy setup.
2.  **(Alternative)** Python 3.11+, PostgreSQL 14+ with the `pgvector` extension installed.
3.  **API Keys**:
    -   `APIFY_API_TOKEN`: For scraping TikTok comments.
    -   `QWEN_API_KEY`: (Optional) For generating RAG answers with an LLM.

## âš™ï¸ Getting Started

### 1. Environment Configuration

Copy the `.env.example` file to a new file named `.env` and fill in your credentials.

```bash
cp .env.example .env
Fill the .env file with your details, similar to the example below:

Code snippet

# ---- Backend ----
LOG_LEVEL=INFO
DATA_DIR=/app/data
# "" (no suffix), AUTO (timestamp), or a custom label:
SAVE_TS_SUFFIX=AUTO

# TikTok scraping
APIFY_API_TOKEN=apify_xxxxxxxxxxxxxxxxxxxxxxx

# Postgres/pgvector (matches docker-compose service names)
PGVECTOR_URL=postgresql://user:pass@doc_pgvector:5432/appdb
PGVECTOR_COLLECTION=comments

# Qwen (optional, OpenAI-compatible)
QWEN_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx
QWEN_BASE_URL=[https://dashscope-intl.aliyuncs.com/compatible-mode/v1](https://dashscope-intl.aliyuncs.com/compatible-mode/v1)

# ---- Frontend ----
BACKEND_URL=http://backend:8000
âš ï¸ Important: Do not commit your .env file to public repositories.

2. Run with Docker (Recommended)
This is the easiest and most consistent method.

Bash

docker compose up -d --build
Once the containers are running, access the application at:

Backend (FastAPI Swagger UI): http://localhost:8000/docs

Frontend (Streamlit): http://localhost:8601

To tail logs from each service:

Bash

docker compose logs -f backend
docker compose logs -f frontend
docker compose logs -f db
If you updated backend code and something feels cached, rebuild the container:

Bash

docker compose build --no-cache backend
docker compose up -d --force-recreate backend
3. Run Locally (No Docker)
Use this method if you prefer not to use Docker. Make sure Postgres with the pgvector extension is already running.

Create and activate a virtual environment:

Bash

python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
Install dependencies:

Bash

pip install -r requirements.txt
Export the same environment variables as in the .env file.

Start the Backend (FastAPI):

Bash

uvicorn backend.api.main:app --host 0.0.0.0 --port 8000 --reload
Start the Frontend (Streamlit) in a separate terminal:

Bash

streamlit run frontend/Home.py --server.port 8601
ğŸ“‚ Project Structure
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/main.py                # FastAPI routes
â”‚   â”œâ”€â”€ graph/pipeline.py          # LangGraph pipeline (scrape -> sentiment -> etc.)
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ scraper.py             # Apify TikTok scraper
â”‚   â”‚   â”œâ”€â”€ preprocessing.py       # IndonesianPreprocessor
â”‚   â”‚   â”œâ”€â”€ sentiment.py           # IndoBERT sentiment
â”‚   â”‚   â”œâ”€â”€ topic.py               # BERTopic + SentenceTransformer
â”‚   â”‚   â”œâ”€â”€ rag.py                 # Hybrid RAG (BM25 + pgvector + Qwen)
â”‚   â”‚   â””â”€â”€ storage.py             # Save files to DATA_DIR
â”‚   â””â”€â”€ utils/db.py                # DB initialization and operations
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ pages/Pipeline_Analisis.py # Page to run the pipeline
â”‚   â”œâ”€â”€ pages/rag.py               # Page for RAG queries
â”‚   â””â”€â”€ Home.py                    # Main Streamlit homepage
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ–¥ï¸ How to Use the Application
ğŸ“„ Pipeline_Analisis Page
Navigate to the Pipeline_Analisis page from the sidebar.

Paste the TikTok URL, a unique Content ID, the Content Date, and the Max Comments to scrape.

Hit the Run Pipeline button.

The application will display:

Insight Summary: Dominant topics and sentiment distribution.

Comments Table: Per-comment analysis results (text, sentiment, topic, etc.).

Download Buttons: Download the results as JSON or CSV files.

ğŸ“š RAG Page
Navigate to the RAG page from the sidebar.

Type your question in the input box (e.g., "What do people think about product X?").

The application will return:

Answer: A summarized answer generated by the Qwen LLM (if QWEN_API_KEY is configured).

Sources: The most relevant comment snippets with their metadata (topic, sentiment, scores).

ğŸ”Œ API Endpoints
Here are the main endpoints available from the FastAPI backend.

POST /analyze
Runs the full analysis pipeline.

Request Body:

JSON

{
  "video_url": "[https://www.tiktok.com/@user/video/123](https://www.tiktok.com/@user/video/123)...",
  "content_id": "indomie",
  "content_date": "2025-09-27",
  "max_comments": 50
}
Response:

JSON

{
  "insight": { "...": "..." },
  "merged_comments_count": 50,
  "merged_comments": [ { "document_id": "indomie", "text": "...", "sentiment": "positive", "confidence": 0.98, "topic_label": "rasa - enak - pedas", "date": "2025-09-27" } ],
  "artifacts": {
    "json": "comments_indomie_20250928-011234.json",
    "csv": "comments_indomie_20250928-011234.csv",
    "insight_txt": "insight_indomie_20250928-011234.txt"
  },
  "rag": {
    "answer": "A summary answer from the LLM...",
    "sources": [{ "...": "..." }]
  }
}
POST /rag/query
Sends a query to the RAG system.

Request Body:

JSON

{
  "query": "What is the audience's main opinion about X?",
  "k": 3
}
Response:

JSON

{
  "answer": "Short answer with [1] [2] citations when relevant",
  "sources": [
    {
      "rank": 1,
      "snippet": "top context ...",
      "document_id": "indomie",
      "topic_label": "rasa - enak - pedas",
      "sentiment": "positive",
      "score_final": 0.75
    }
  ]
}
GET /files/{type}/{content_id}.{ext}
Downloads the latest analysis file from the server.

GET /files/comments/indomie.csv

GET /files/insight/indomie.txt

ğŸ”§ Troubleshooting
RAG returns [No LLM]: You didnâ€™t set QWEN_API_KEY. Thatâ€™s fine if you just want search + sources without generated prose.

AttributeError in the backend: Your backend container is stale. Rebuild it and clear pyc files (see the "Run with Docker" section).

Streamlit error "Expanders may not be nested": Keep the sources listing inside one expander; donâ€™t open another expander inside it.

Apify returns 0 comments: Try another TikTok video, ensure the actor is allowed to scrape that content, or increase max_comments.

File download 404: Make sure the pipeline produced the files (check /app/data in the backend container).

ğŸ¤ Contributing
PRs are very welcome!

Keep the docs friendly for first-time users.

Avoid nested expanders in Streamlit.

Small, focused changes are easiest to review.

ğŸ“œ License
This project is licensed under the MIT License. See the LICENSE file for details.
